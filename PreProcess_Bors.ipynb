{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in /home/hamed/anaconda3/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: libwapiti>=0.2.1; platform_system != \"Windows\" in /home/hamed/anaconda3/lib/python3.8/site-packages (from hazm) (0.2.1)\n",
      "Requirement already satisfied: nltk==3.3 in /home/hamed/anaconda3/lib/python3.8/site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in /home/hamed/anaconda3/lib/python3.8/site-packages (from libwapiti>=0.2.1; platform_system != \"Windows\"->hazm) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import json\n",
    "import pandas as pd\n",
    "from hazm import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tagger = POSTagger(model='/mnt/d/BigData/resources/postagger.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_keywords = ['بورس','اقتصاد','تحریم','دولت','حسن روحانی','انتخابات','دلار','طلا','کرونا','تورم','دانشگاه','کووید۱۹','کوید۱۹','کویید۱۹','کویید ۱۹','کوید ۱۹']\n",
    "special_keywords = set(special_keywords)\n",
    "stop_word_set = set(stopwords_list())\n",
    "def extract_verbs(tokenized_sentence):\n",
    "    verb_list = set()\n",
    "    for word,tag in tokenized_sentence:\n",
    "        if tag=='V':\n",
    "            verb_list.add(word)\n",
    "    return list(verb_list)\n",
    "\n",
    "def keyWordExtraction(doc):\n",
    "    text=doc['content']\n",
    "    normalizer = Normalizer()\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, preprocessor=normalizer.normalize, tokenizer=word_tokenize,\n",
    "                                 stop_words=stopwords_list())\n",
    "    tokenized_sentence = tagger.tag(word_tokenize(text.replace('#','')))\n",
    "    verbs = extract_verbs(tokenized_sentence)\n",
    "    for v in verbs: # Remove Verbs From Text\n",
    "        text = text.replace(v,'')\n",
    "    vectors = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    denselist = vectors.todense().tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n",
    "    sp_set = set()\n",
    "    for cl in df.columns:\n",
    "        if cl.replace('#','') in special_keywords:\n",
    "            df[cl]=0\n",
    "            sp_set.add(cl.replace('#',''))\n",
    "    df.sort_values(by=0, axis=1, ascending=False, inplace=True, na_position='last')\n",
    "    max = df.values.size\n",
    "    min = int(max * 0.70)\n",
    "    keywords = list(df.keys()[min:max].values)\n",
    "    keywords = [x.replace('#','') for x in keywords]\n",
    "    keywords = [x for x in keywords if len(x) >= 2 and x not in stop_word_set]\n",
    "    doc['keywords']= list(set(keywords+ list(sp_set) ))\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'PreProcess',\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "def preprocess(tweet):\n",
    "    tweet['hashtags'] = re.findall(r\"#(\\w+)\",tweet['content'])\n",
    "    tweet['links']= extractor.find_urls(tweet['content'])\n",
    "    tweet = keyWordExtraction(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],key_serializer=str.encode,\n",
    "                         value_serializer=lambda x:dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt=0\n",
    "ext_counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3100\n",
      "3150\n",
      "3200\n",
      "3250\n",
      "3300\n",
      "3350\n",
      "3400\n",
      "3450\n",
      "3500\n",
      "3550\n",
      "3600\n",
      "3650\n",
      "3700\n",
      "3750\n",
      "3800\n",
      "3850\n",
      "3900\n",
      "3950\n",
      "4000\n",
      "4050\n",
      "4100\n",
      "4150\n",
      "4200\n",
      "4250\n",
      "4300\n",
      "4350\n",
      "4400\n",
      "4450\n",
      "4500\n",
      "4550\n",
      "4600\n",
      "4650\n",
      "4700\n",
      "4750\n",
      "4800\n",
      "4850\n",
      "4900\n",
      "4950\n",
      "5000\n",
      "5050\n",
      "5100\n",
      "5150\n",
      "5200\n",
      "5250\n",
      "5300\n",
      "5350\n",
      "5400\n",
      "5450\n",
      "5500\n",
      "5550\n",
      "5600\n",
      "5650\n",
      "5700\n",
      "5750\n",
      "5800\n",
      "5850\n",
      "5900\n",
      "5950\n",
      "6000\n",
      "6050\n",
      "6100\n",
      "6150\n",
      "6200\n",
      "6250\n",
      "6300\n",
      "6350\n",
      "6400\n",
      "6450\n",
      "6500\n",
      "6550\n",
      "6600\n",
      "6650\n",
      "6700\n",
      "6750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in consumer:\n",
    "    cnt+=1\n",
    "#     if cnt < 100:\n",
    "#         continue\n",
    "    if cnt%50 ==0:\n",
    "        print(cnt)\n",
    "    message = message.value\n",
    "    if 'content' not in message or len(message['content'])< 10:\n",
    "        continue\n",
    "#     print('t2')\n",
    "    try:\n",
    "        tweet = preprocess(message)\n",
    "    #     print('t3')\n",
    "        producer.send('persistence', key=tweet['id'],value=tweet)\n",
    "        producer.send('statics', key=tweet['id'],value=tweet)\n",
    "    except:\n",
    "        ext_counter+=1\n",
    "        if ext_counter % 50 ==0:\n",
    "            print(\"An exception occurred: \"+str(ext_counter)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
