{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 473 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 802 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting libwapiti>=0.2.1; platform_system != \"Windows\"\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/hamed/anaconda3/lib/python3.8/site-packages (from nltk==3.3->hazm) (1.15.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394470 sha256=b2a220fc67984100926ff8bf29ce6f42037770d4fcbaf641a918f5179c09ba94\n",
      "  Stored in directory: /home/hamed/.cache/pip/wheels/19/1d/3a/0a8c14c30132b4f9ffd796efbb6746f15b3d6bcfc1055a9346\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp38-cp38-linux_x86_64.whl size=167516 sha256=fd87a15cc2533afb4cad5648aa7245c4998ada8e1a6824587458bb9e3cd4a5d2\n",
      "  Stored in directory: /home/hamed/.cache/pip/wheels/3c/d8/9f/59fd78b2b7d1e9ffcb68fb6de80c2e7c20b804c8cbc4d8fc23\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.5\n",
      "    Uninstalling nltk-3.5:\n",
      "      Successfully uninstalled nltk-3.5\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting urlextract\n",
      "  Downloading urlextract-1.3.0-py3-none-any.whl (19 kB)\n",
      "Collecting uritools\n",
      "  Downloading uritools-3.0.2-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: filelock in /home/hamed/anaconda3/lib/python3.8/site-packages (from urlextract) (3.0.12)\n",
      "Requirement already satisfied: idna in /home/hamed/anaconda3/lib/python3.8/site-packages (from urlextract) (2.10)\n",
      "Collecting appdirs\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Installing collected packages: uritools, appdirs, urlextract\n",
      "Successfully installed appdirs-1.4.4 uritools-3.0.2 urlextract-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install urlextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[K     |████████████████████████████████| 246 kB 465 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from json import loads\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "from urlextract import URLExtract\n",
    "import json\n",
    "import pandas as pd\n",
    "from hazm import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tagger = POSTagger(model='/mnt/d/BigData/resources/postagger.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_keywords = ['بورس','اقتصاد','تحریم','دولت','حسن روحانی','انتخابات','دلار','طلا','کرونا','تورم','دانشگاه','کووید۱۹','کوید۱۹','کویید۱۹','کویید ۱۹','کوید ۱۹']\n",
    "special_keywords = set(special_keywords)\n",
    "\n",
    "def extract_verbs(tokenized_sentence):\n",
    "    verb_list = set()\n",
    "    for word,tag in tokenized_sentence:\n",
    "        if tag=='V':\n",
    "            verb_list.add(word)\n",
    "    return list(verb_list)\n",
    "\n",
    "def keyWordExtraction(doc):\n",
    "    text=doc['content']\n",
    "    normalizer = Normalizer()\n",
    "    vectorizer = TfidfVectorizer(lowercase=False, preprocessor=normalizer.normalize, tokenizer=word_tokenize,\n",
    "                                 stop_words=stopwords_list())\n",
    "    tokenized_sentence = tagger.tag(word_tokenize(text.replace('#','')))\n",
    "    verbs = extract_verbs(tokenized_sentence)\n",
    "    for v in verbs: # Remove Verbs From Text\n",
    "        text = text.replace(v,'')\n",
    "    vectors = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    denselist = vectors.todense().tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    \n",
    "    for cl in df.columns:\n",
    "        if cl.replace('#','') in special_keywords:\n",
    "            df[cl]=0\n",
    "    df.sort_values(by=0, axis=1, ascending=False, inplace=True, na_position='last')\n",
    "    max = df.values.size\n",
    "    min = int(max * 0.6)\n",
    "    keywords = list(df.keys()[min:max].values)\n",
    "    keywords = [x.replace('#','') for x in keywords]\n",
    "    doc['keywords']= keywords\n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(\n",
    "    'PreProcess',\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='earliest',\n",
    "     enable_auto_commit=True,\n",
    "     group_id='my-group',\n",
    "     value_deserializer=lambda x: loads(x.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = URLExtract()\n",
    "def preprocess(tweet):\n",
    "    tweet['hashtags'] = re.findall(r\"#(\\w+)\",tweet['content'])\n",
    "    tweet['links']= extractor.find_urls(tweet['content'])\n",
    "    tweet = keyWordExtraction(tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'],key_serializer=str.encode,\n",
    "                         value_serializer=lambda x:dumps(x).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    message = message.value\n",
    "    tweet = preprocess(message)\n",
    "    producer.send('persistence', key=tweet['id'],value=tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
